{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_index(train):\n",
    "   \"\"\"\n",
    "   Preprocesses the training data and returns indices of valid samples after filtering.\n",
    "   This function is similar to preprocess_data but includes additional data cleaning steps\n",
    "   and returns only the indices of valid samples.\n",
    "   \"\"\"\n",
    "   # Remove unnecessary identifier columns\n",
    "   train = train.drop(['id', 'Name'], axis=1)\n",
    "   \n",
    "   # Combine Work and Academic Pressure into a single pressure metric\n",
    "   # Takes the maximum value between the two types of pressure\n",
    "   train['Pressure'] = train[['Work Pressure', 'Academic Pressure']].max(axis=1)\n",
    "   train = train.drop(['Work Pressure', 'Academic Pressure'], axis=1)\n",
    "   \n",
    "   # Convert Gender to binary (1 for Male, 0 for Female)\n",
    "   train['Gender'] = (train['Gender'] == 'Male').astype(int)\n",
    "   \n",
    "   # Set Profession to 'Student' where the person is a student\n",
    "   # Note: Commented code shows alternative binary encoding approach\n",
    "   # train['Working Professional or Student'] = (train['Working Professional or Student'] == 'Working Professional').astype(int)\n",
    "   train.loc[train['Working Professional or Student'] == 'Student', 'Profession'] = 'Student'\n",
    "   \n",
    "   # Combine Study and Job Satisfaction into a single satisfaction metric\n",
    "   train['Satisfaction'] = train[['Study Satisfaction', 'Job Satisfaction']].max(axis=1)\n",
    "   train = train.drop(['Study Satisfaction', 'Job Satisfaction'], axis=1)\n",
    "   \n",
    "   # Convert Yes/No columns to binary (1 for Yes, 0 for No)\n",
    "   train['Family History of Mental Illness'] = (train['Family History of Mental Illness'] == 'Yes').astype(int)\n",
    "   train['Have you ever had suicidal thoughts ?'] = (train['Have you ever had suicidal thoughts ?'] == 'Yes').astype(int)\n",
    "   \n",
    "   # Remove City column (alternative would be one-hot encoding)\n",
    "   # Commented code shows one-hot encoding approach:\n",
    "   # train = pd.get_dummies(train, columns=['City']).astype(int)\n",
    "   train = train.drop(['City'], axis=1)\n",
    "   \n",
    "   # Map dietary habits to numerical values and filter out invalid values\n",
    "   diet_mapping = {'Moderate': 1.0, 'Unhealthy': 0.0, 'Healthy': 2.0}\n",
    "   train = train[train['Dietary Habits'].isin(diet_mapping.keys())]  # Remove rows with invalid dietary habits\n",
    "   train['Dietary Habits'] = train['Dietary Habits'].map(diet_mapping)\n",
    "   \n",
    "   # Filter professions with less than 10 samples to ensure statistical significance\n",
    "   v = train[\"Profession\"].value_counts() \n",
    "   train = train[train['Profession'].isin(v.index[v.gt(10)])]\n",
    "   \n",
    "   # Convert Profession column to one-hot encoded columns\n",
    "   train = pd.get_dummies(train, columns=['Profession'])\n",
    "   profession_cols = [col for col in train.columns if col.startswith('Profession_')]\n",
    "   train[profession_cols] = train[profession_cols].astype(int)\n",
    "   \n",
    "   # Remove redundant column since profession is now one-hot encoded\n",
    "   train = train.drop(['Working Professional or Student'], axis=1)\n",
    "   \n",
    "   # Filter degrees with less than 10 samples to ensure statistical significance\n",
    "   v = train[\"Degree\"].value_counts() \n",
    "   train = train[train['Degree'].isin(v.index[v.gt(10)])]\n",
    "   \n",
    "   # Convert Degree column to one-hot encoded columns\n",
    "   train = pd.get_dummies(train, columns=['Degree'])\n",
    "   degree_cols = [col for col in train.columns if col.startswith('Degree_')]\n",
    "   train[degree_cols] = train[degree_cols].astype(int)\n",
    "   \n",
    "   # Map sleep duration ranges to their approximate middle values in hours\n",
    "   # Also filter out invalid sleep duration values\n",
    "   dict_sleep = {\n",
    "       'Less than 5 hours': 4.0, \n",
    "       '5-6 hours': 5.5, \n",
    "       '6-7 hours': 6.5, \n",
    "       '7-8 hours': 7.5, \n",
    "       'More than 8 hours': 9.0,\n",
    "       '2-3 hours': 2.5,\n",
    "       '3-4 hours': 3.5,\n",
    "       '4-5 hours': 4.5,\n",
    "       '4-6 hours': 5.0\n",
    "   }\n",
    "   train = train[train['Sleep Duration'].isin(dict_sleep.keys())]\n",
    "   train['Sleep Duration'] = train['Sleep Duration'].map(dict_sleep)\n",
    "   \n",
    "   # Fill missing CGPA values with mean and remove any remaining NA values\n",
    "   train['CGPA'] = train['CGPA'].fillna(train['CGPA'].mean())\n",
    "   train = train.dropna()\n",
    "   \n",
    "   # Return indices of valid samples after all filtering steps\n",
    "   return train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(train):\n",
    "    # Remove unnecessary identifier columns\n",
    "    train = train.drop(['id', 'Name'], axis=1)\n",
    "    \n",
    "    # Combine Work and Academic Pressure into a single pressure metric\n",
    "    # Takes the maximum value between the two types of pressure\n",
    "    train['Pressure'] = train[['Work Pressure', 'Academic Pressure']].max(axis=1)\n",
    "    train = train.drop(['Work Pressure', 'Academic Pressure'], axis=1)\n",
    "    \n",
    "    # Convert Gender to binary (1 for Male, 0 for Female)\n",
    "    train['Gender'] = (train['Gender'] == 'Male').astype(int)\n",
    "    \n",
    "    # Set Profession to 'Student' where the person is a student\n",
    "    train.loc[train['Working Professional or Student'] == 'Student', 'Profession'] = 'Student'\n",
    "    \n",
    "    # Combine Study and Job Satisfaction into a single satisfaction metric\n",
    "    # Takes the maximum value between the two types of satisfaction\n",
    "    train['Satisfaction'] = train[['Study Satisfaction', 'Job Satisfaction']].max(axis=1)\n",
    "    train = train.drop(['Study Satisfaction', 'Job Satisfaction'], axis=1)\n",
    "    \n",
    "    # Convert Yes/No columns to binary (1 for Yes, 0 for No)\n",
    "    train['Family History of Mental Illness'] = (train['Family History of Mental Illness'] == 'Yes').astype(int)\n",
    "    train['Have you ever had suicidal thoughts ?'] = (train['Have you ever had suicidal thoughts ?'] == 'Yes').astype(int)\n",
    "    \n",
    "    # Remove City column as it's not relevant for the analysis\n",
    "    train = train.drop(['City'], axis=1)\n",
    "    \n",
    "    # Map dietary habits to numerical values\n",
    "    # 0.0 = Unhealthy, 1.0 = Moderate, 2.0 = Healthy\n",
    "    diet_mapping = {'Moderate': 1.0, 'Unhealthy': 0.0, 'Healthy': 2.0}\n",
    "    train['Dietary Habits'] = train['Dietary Habits'].map(diet_mapping)\n",
    "    \n",
    "    # Convert Profession column to one-hot encoded columns\n",
    "    train = pd.get_dummies(train, columns=['Profession'])\n",
    "    \n",
    "    # Ensure all profession columns are integer type\n",
    "    profession_cols = [col for col in train.columns if col.startswith('Profession_')]\n",
    "    train[profession_cols] = train[profession_cols].astype(int)\n",
    "    \n",
    "    # Remove redundant column since profession is now one-hot encoded\n",
    "    train = train.drop(['Working Professional or Student'], axis=1)\n",
    "    \n",
    "    # Convert Degree column to one-hot encoded columns\n",
    "    train = pd.get_dummies(train, columns=['Degree'])\n",
    "    \n",
    "    # Ensure all degree columns are integer type\n",
    "    degree_cols = [col for col in train.columns if col.startswith('Degree_')]\n",
    "    train[degree_cols] = train[degree_cols].astype(int)\n",
    "    \n",
    "    # Map sleep duration ranges to their approximate middle values in hours\n",
    "    dict_sleep = {\n",
    "        'Less than 5 hours': 4.0,\n",
    "        '5-6 hours': 5.5,\n",
    "        '6-7 hours': 6.5,\n",
    "        '7-8 hours': 7.5,\n",
    "        'More than 8 hours': 9.0,\n",
    "        '2-3 hours': 2.5,\n",
    "        '3-4 hours': 3.5,\n",
    "        '4-5 hours': 4.5,\n",
    "        '4-6 hours': 5.0\n",
    "    }\n",
    "    train['Sleep Duration'] = train['Sleep Duration'].map(dict_sleep)\n",
    "    \n",
    "    # Fill missing CGPA values with the mean CGPA\n",
    "    train['CGPA'] = train['CGPA'].fillna(train['CGPA'].mean())\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path='', PCA=False, n_components=0.95):\n",
    "   \"\"\"\n",
    "   Loads data from CSV files, preprocesses it, and optionally applies PCA dimensionality reduction.\n",
    "   \n",
    "   Parameters:\n",
    "   -----------\n",
    "   path : str\n",
    "       Path to the directory containing train.csv and test.csv files\n",
    "   PCA : bool\n",
    "       Whether to apply PCA dimensionality reduction\n",
    "   n_components : float or int\n",
    "       Number of components for PCA if PCA=True\n",
    "       If float (0-1), represents the percentage of variance to preserve\n",
    "       If int (>1), represents the number of components to keep\n",
    "       \n",
    "   Returns:\n",
    "   --------\n",
    "   X_train : DataFrame or numpy array\n",
    "       Preprocessed training features \n",
    "   y_train : Series\n",
    "       Training target values (Depression)\n",
    "   X_test : DataFrame or numpy array\n",
    "       Preprocessed test features\n",
    "   \"\"\"\n",
    "   # Load the raw data\n",
    "   train = pd.read_csv(path + 'train.csv')\n",
    "   test = pd.read_csv(path + 'test.csv')\n",
    "   \n",
    "   # Separate features and target from training data\n",
    "   X_train = train.drop('Depression', axis=1)\n",
    "   y_train = train['Depression']\n",
    "   \n",
    "   # Get valid indices after preprocessing\n",
    "   X_train_index = preprocess_data_index(X_train)\n",
    "   # Filter training data to keep only valid samples\n",
    "   X_train = X_train.loc[X_train_index]\n",
    "   \n",
    "   # Store length of training data for later splitting\n",
    "   len_train = len(X_train)\n",
    "   \n",
    "   # Combine train and test for consistent preprocessing\n",
    "   X = pd.concat([X_train, test], axis=0)\n",
    "   \n",
    "   # Apply preprocessing to combined data\n",
    "   X = preprocess_data(X)\n",
    "   \n",
    "   # Split back into train and test\n",
    "   X_train = X[:len_train]\n",
    "   X_test = X[len_train:]\n",
    "   \n",
    "   # Fill missing values in test set with mean values\n",
    "   X_test = X_test.fillna(X_test.mean())\n",
    "   \n",
    "   # Recombine features and target for final cleaning\n",
    "   train = pd.concat([X_train, y_train], axis=1)\n",
    "   train = train.dropna()\n",
    "   \n",
    "   # Final split of clean training data\n",
    "   X_train = train.drop('Depression', axis=1)\n",
    "   y_train = train['Depression']\n",
    "   \n",
    "   # Optionally apply PCA\n",
    "   if PCA:\n",
    "       pca = PCA(n_components=n_components)\n",
    "       X_train = pca.fit_transform(X_train)\n",
    "       X_test = pca.transform(X_test)\n",
    "   \n",
    "   return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(X_train = X_train, y_train = y_train):\n",
    "    # Split the data into Student data and Professional data\n",
    "    X_trainS = X_train[X_train['Profession_Student'] == 1]\n",
    "    X_trainP = X_train[X_train['Profession_Student'] == 0]\n",
    "    X_trainS = X_trainS.drop(['Profession_Student'], axis=1)\n",
    "    X_trainP = X_trainP.drop(['Profession_Student'], axis=1)\n",
    "    y_trainS = y_train[X_trainS.index]\n",
    "    y_trainP = y_train[X_trainP.index]\n",
    "\n",
    "    X_trainS2, X_valS, y_trainS2, y_valS = train_test_split(X_trainS, y_trainS, test_size=0.2, random_state=42)\n",
    "    X_trainS2 = pd.concat([X_trainS2, X_trainP], axis=0)\n",
    "    y_trainS2 = pd.concat([y_trainS2, y_trainP], axis=0)\n",
    "    X_trainP2, X_valP, y_trainP2, y_valP = train_test_split(X_trainP, y_trainP, test_size=0.2, random_state=42)\n",
    "    return X_trainS2, y_trainS2, X_valS, y_valS, X_trainP2, y_trainP2, X_valP, y_valP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model (model, X_train = X_train, y_train = y_train, scoring = {'accuracy': accuracy_score, 'balanced_accuracy': balanced_accuracy_score, 'roc_auc': roc_auc_score}):\n",
    "    X_trainS2, y_trainS2, X_valS, y_valS, X_trainP2, y_trainP2, X_valP, y_valP = SplitData(X_train, y_train)\n",
    "    X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_trainS2, y_trainS2 = sm.fit_resample(X_trainS2, y_trainS2)\n",
    "    X_trainP2, y_trainP2 = sm.fit_resample(X_trainP2, y_trainP2)\n",
    "    X_train2, y_train2 = sm.fit_resample(X_train2, y_train2)\n",
    "\n",
    "    model1 = clone(model)\n",
    "    model2 = clone(model)\n",
    "    model3 = clone(model)\n",
    "    model1.fit(X_trainS2, y_trainS2)\n",
    "    model2.fit(X_trainP2, y_trainP2)\n",
    "    model3.fit(X_train2, y_train2)\n",
    "    y_predS = model1.predict(X_valS)\n",
    "    y_predP = model2.predict(X_valP)\n",
    "    Student_score = {}\n",
    "    Professional_score = {}\n",
    "    Combined_score = {}\n",
    "    No_split_score = {}\n",
    "    for name, value in scoring.items():\n",
    "        tmp1 = value(y_valS, y_predS)\n",
    "        Student_score[name] = tmp1\n",
    "        print(f'Student {name} = {tmp1}')\n",
    "        tmp2 = value(y_valP, y_predP)\n",
    "        Professional_score[name] = tmp2\n",
    "        print(f'Professional {name} = {tmp2}')\n",
    "        y_pred_combined = np.concatenate((y_predS, y_predP))\n",
    "        y_val_combined = np.concatenate((y_valS, y_valP))\n",
    "        tmp3 = value(y_val_combined, y_pred_combined)\n",
    "        print(f'Combined {name} = {tmp3}')\n",
    "        Combined_score[name] = tmp3\n",
    "        y_pred = model3.predict(X_val)\n",
    "        tmp4 = value(y_val, y_pred)\n",
    "        print(f'No split {name} = {tmp4}')\n",
    "        No_split_score[name] = tmp4\n",
    "\n",
    "\n",
    "\n",
    "    return Student_score, Professional_score, Combined_score, No_split_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_submition(model, X_test = X_test, test = pd.read_csv('test.csv')):\n",
    "    testS = X_test[X_test['Profession_Student'] == 1]\n",
    "    testP = X_test[X_test['Profession_Student'] == 0]\n",
    "    testS = testS.drop(['Profession_Student'], axis=1)\n",
    "    testP = testP.drop(['Profession_Student'], axis=1)\n",
    "    \n",
    "    X_trainS = X_train[X_train['Profession_Student'] == 1]\n",
    "    X_trainP = X_train[X_train['Profession_Student'] == 0]\n",
    "    X_trainS = X_trainS.drop(['Profession_Student'], axis=1)\n",
    "    X_trainP = X_trainP.drop(['Profession_Student'], axis=1)\n",
    "    y_trainS = y_train[X_trainS.index]\n",
    "    y_trainP = y_train[X_trainP.index]\n",
    "    X_train3 = X_train.drop(['Profession_Student'], axis=1)\n",
    "\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train3, y_train = sm.fit_resample(X_train3, y_train)\n",
    "    model.fit(X_train3, y_train)\n",
    "    y_predS = model.predict(testS)\n",
    "    X_trainP, y_trainP = sm.fit_resample(X_trainP, y_trainP)\n",
    "    model.fit(X_trainP, y_trainP)\n",
    "    y_predP = model.predict(testP)\n",
    "    y_pred = np.zeros(len(X_test))\n",
    "    y_pred[testS.index] = y_predS\n",
    "    y_pred[testP.index] = y_predP\n",
    "\n",
    "    submission = pd.DataFrame({'id': test['id'], 'Depression': y_pred})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karim/miniconda3/envs/T1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/karim/miniconda3/envs/T1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student accuracy = 0.7913074712643678\n",
      "Professional accuracy = 0.9102243188601136\n",
      "Combined accuracy = 0.8850884519019057\n",
      "No split accuracy = 0.9215701161643004\n",
      "Student balanced_accuracy = 0.7608741534000131\n",
      "Professional balanced_accuracy = 0.9001196109594847\n",
      "Combined balanced_accuracy = 0.9087636724404886\n",
      "No split balanced_accuracy = 0.9210491613007161\n",
      "Student roc_auc = 0.7608741534000132\n",
      "Professional roc_auc = 0.9001196109594846\n",
      "Combined roc_auc = 0.9087636724404885\n",
      "No split roc_auc = 0.9210491613007161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karim/miniconda3/envs/T1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=100)\n",
    "\n",
    "Student_score, Professional_score, Combined_score, No_split_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_results():\n",
    "    # Dictionary of models with their parameters\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            max_depth=10, max_features=None, min_samples_leaf=4,\n",
    "            min_samples_split=10, n_estimators=200, random_state=42\n",
    "        ),\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            C=100, penalty='l1', solver='liblinear', max_iter=10000, random_state=42\n",
    "        ),\n",
    "        'KNN': KNeighborsClassifier(\n",
    "            algorithm='ball_tree', metric='manhattan', n_neighbors=11, weights='uniform'\n",
    "        ),\n",
    "        'DecisionTree': DecisionTreeClassifier(\n",
    "            criterion='entropy', max_depth=10, min_samples_leaf=4,\n",
    "            min_samples_split=10, random_state=42\n",
    "        ),\n",
    "        'GradientBoosting': GradientBoostingClassifier(\n",
    "            learning_rate=0.1, max_depth=3, n_estimators=300,\n",
    "            subsample=0.8, random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            colsample_bytree=1.0, learning_rate=0.1, max_depth=3,\n",
    "            n_estimators=300, subsample=0.8, random_state=42\n",
    "        ),\n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            depth=4, iterations=200, l2_leaf_reg=5,\n",
    "            learning_rate=0.2, random_state=42, verbose=False\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    results = []\n",
    "    \n",
    "    # Metrics to evaluate\n",
    "    scoring = {\n",
    "        'accuracy': accuracy_score,\n",
    "        'balanced_accuracy': balanced_accuracy_score,\n",
    "        'f1': f1_score,\n",
    "        'recall': recall_score,\n",
    "        'roc_auc': roc_auc_score\n",
    "    }\n",
    "    \n",
    "    # Test each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        student_score, prof_score, combined_score, no_split_score = test_model(\n",
    "            model, X_train, y_train, scoring\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Hyperparameters': str(model.get_params()),\n",
    "            'Balanced_accuracy_Student': student_score['balanced_accuracy'],\n",
    "            'Balanced_accuracy_Professional': prof_score['balanced_accuracy'],\n",
    "            'Balanced_accuracy_Combined': combined_score['balanced_accuracy'],\n",
    "            'Accuracy_Student': student_score['accuracy'],\n",
    "            'Accuracy_Professional': prof_score['accuracy'],\n",
    "            'Accuracy_Combined': combined_score['accuracy'],\n",
    "            'F1_Student': student_score['f1'],\n",
    "            'F1_Professional': prof_score['f1'],\n",
    "            'F1_Combined': combined_score['f1'],\n",
    "            'Recall_Student': student_score['recall'],\n",
    "            'Recall_Professional': prof_score['recall'],\n",
    "            'Recall_Combined': combined_score['recall'],\n",
    "            'ROC_AUC_Student': student_score['roc_auc'],\n",
    "            'ROC_AUC_Professional': prof_score['roc_auc'],\n",
    "            'ROC_AUC_Combined': combined_score['roc_auc']\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Print best models for each metric\n",
    "    metrics = ['Balanced_accuracy', 'Accuracy', 'F1', 'Recall', 'ROC_AUC']\n",
    "    categories = ['Student', 'Professional', 'Combined']\n",
    "    \n",
    "    print(\"\\nBest Models for Each Metric:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        for category in categories:\n",
    "            column = f\"{metric}_{category}\"\n",
    "            best_idx = results_df[column].idxmax()\n",
    "            best_model = results_df.loc[best_idx, 'Model']\n",
    "            best_score = results_df.loc[best_idx, column]\n",
    "            print(f\"{category}: {best_model} (Score: {best_score:.4f})\")\n",
    "    \n",
    "    \n",
    "    return results_df\n",
    "\n",
    "results_df = create_model_results()\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Balanced_accuracy_Student</th>\n",
       "      <th>Balanced_accuracy_Professional</th>\n",
       "      <th>Balanced_accuracy_Combined</th>\n",
       "      <th>Accuracy_Student</th>\n",
       "      <th>Accuracy_Professional</th>\n",
       "      <th>Accuracy_Combined</th>\n",
       "      <th>F1_Student</th>\n",
       "      <th>F1_Professional</th>\n",
       "      <th>F1_Combined</th>\n",
       "      <th>Recall_Student</th>\n",
       "      <th>Recall_Professional</th>\n",
       "      <th>Recall_Combined</th>\n",
       "      <th>ROC_AUC_Student</th>\n",
       "      <th>ROC_AUC_Professional</th>\n",
       "      <th>ROC_AUC_Combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n",
       "      <td>0.816140</td>\n",
       "      <td>0.853895</td>\n",
       "      <td>0.899963</td>\n",
       "      <td>0.829921</td>\n",
       "      <td>0.949937</td>\n",
       "      <td>0.924569</td>\n",
       "      <td>0.859891</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.794540</td>\n",
       "      <td>0.909262</td>\n",
       "      <td>0.744630</td>\n",
       "      <td>0.862789</td>\n",
       "      <td>0.816140</td>\n",
       "      <td>0.853895</td>\n",
       "      <td>0.899963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 100, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.755660</td>\n",
       "      <td>0.899093</td>\n",
       "      <td>0.910628</td>\n",
       "      <td>0.788254</td>\n",
       "      <td>0.909695</td>\n",
       "      <td>0.884026</td>\n",
       "      <td>0.841041</td>\n",
       "      <td>0.543108</td>\n",
       "      <td>0.734878</td>\n",
       "      <td>0.975907</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.755660</td>\n",
       "      <td>0.899093</td>\n",
       "      <td>0.910628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'algorithm': 'ball_tree', 'leaf_size': 30, 'm...</td>\n",
       "      <td>0.729456</td>\n",
       "      <td>0.861442</td>\n",
       "      <td>0.894672</td>\n",
       "      <td>0.761853</td>\n",
       "      <td>0.922162</td>\n",
       "      <td>0.888277</td>\n",
       "      <td>0.820520</td>\n",
       "      <td>0.551953</td>\n",
       "      <td>0.732382</td>\n",
       "      <td>0.948373</td>\n",
       "      <td>0.792363</td>\n",
       "      <td>0.904334</td>\n",
       "      <td>0.729456</td>\n",
       "      <td>0.861442</td>\n",
       "      <td>0.894672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.806333</td>\n",
       "      <td>0.847767</td>\n",
       "      <td>0.892611</td>\n",
       "      <td>0.820223</td>\n",
       "      <td>0.944017</td>\n",
       "      <td>0.917850</td>\n",
       "      <td>0.851813</td>\n",
       "      <td>0.614773</td>\n",
       "      <td>0.778596</td>\n",
       "      <td>0.900188</td>\n",
       "      <td>0.738266</td>\n",
       "      <td>0.854480</td>\n",
       "      <td>0.806333</td>\n",
       "      <td>0.847767</td>\n",
       "      <td>0.892611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...</td>\n",
       "      <td>0.835599</td>\n",
       "      <td>0.819615</td>\n",
       "      <td>0.895810</td>\n",
       "      <td>0.844648</td>\n",
       "      <td>0.966641</td>\n",
       "      <td>0.940855</td>\n",
       "      <td>0.868880</td>\n",
       "      <td>0.702958</td>\n",
       "      <td>0.825532</td>\n",
       "      <td>0.896746</td>\n",
       "      <td>0.652347</td>\n",
       "      <td>0.827757</td>\n",
       "      <td>0.835599</td>\n",
       "      <td>0.819615</td>\n",
       "      <td>0.895810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'objective': 'binary:logistic', 'base_score':...</td>\n",
       "      <td>0.835803</td>\n",
       "      <td>0.824132</td>\n",
       "      <td>0.897332</td>\n",
       "      <td>0.845007</td>\n",
       "      <td>0.966737</td>\n",
       "      <td>0.941007</td>\n",
       "      <td>0.869302</td>\n",
       "      <td>0.706582</td>\n",
       "      <td>0.826524</td>\n",
       "      <td>0.897997</td>\n",
       "      <td>0.661893</td>\n",
       "      <td>0.831350</td>\n",
       "      <td>0.835803</td>\n",
       "      <td>0.824132</td>\n",
       "      <td>0.897332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'iterations': 200, 'learning_rate': 0.2, 'dep...</td>\n",
       "      <td>0.840087</td>\n",
       "      <td>0.805550</td>\n",
       "      <td>0.892489</td>\n",
       "      <td>0.848240</td>\n",
       "      <td>0.966785</td>\n",
       "      <td>0.941728</td>\n",
       "      <td>0.871326</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.825983</td>\n",
       "      <td>0.895181</td>\n",
       "      <td>0.622116</td>\n",
       "      <td>0.818100</td>\n",
       "      <td>0.840087</td>\n",
       "      <td>0.805550</td>\n",
       "      <td>0.892489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                                    Hyperparameters  \\\n",
       "0        RandomForest  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...   \n",
       "1  LogisticRegression  {'C': 100, 'class_weight': None, 'dual': False...   \n",
       "2                 KNN  {'algorithm': 'ball_tree', 'leaf_size': 30, 'm...   \n",
       "3        DecisionTree  {'ccp_alpha': 0.0, 'class_weight': None, 'crit...   \n",
       "4    GradientBoosting  {'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...   \n",
       "5             XGBoost  {'objective': 'binary:logistic', 'base_score':...   \n",
       "6            CatBoost  {'iterations': 200, 'learning_rate': 0.2, 'dep...   \n",
       "\n",
       "   Balanced_accuracy_Student  Balanced_accuracy_Professional  \\\n",
       "0                   0.816140                        0.853895   \n",
       "1                   0.755660                        0.899093   \n",
       "2                   0.729456                        0.861442   \n",
       "3                   0.806333                        0.847767   \n",
       "4                   0.835599                        0.819615   \n",
       "5                   0.835803                        0.824132   \n",
       "6                   0.840087                        0.805550   \n",
       "\n",
       "   Balanced_accuracy_Combined  Accuracy_Student  Accuracy_Professional  \\\n",
       "0                    0.899963          0.829921               0.949937   \n",
       "1                    0.910628          0.788254               0.909695   \n",
       "2                    0.894672          0.761853               0.922162   \n",
       "3                    0.892611          0.820223               0.944017   \n",
       "4                    0.895810          0.844648               0.966641   \n",
       "5                    0.897332          0.845007               0.966737   \n",
       "6                    0.892489          0.848240               0.966785   \n",
       "\n",
       "   Accuracy_Combined  F1_Student  F1_Professional  F1_Combined  \\\n",
       "0           0.924569    0.859891         0.642857     0.794540   \n",
       "1           0.884026    0.841041         0.543108     0.734878   \n",
       "2           0.888277    0.820520         0.551953     0.732382   \n",
       "3           0.917850    0.851813         0.614773     0.778596   \n",
       "4           0.940855    0.868880         0.702958     0.825532   \n",
       "5           0.941007    0.869302         0.706582     0.826524   \n",
       "6           0.941728    0.871326         0.693878     0.825983   \n",
       "\n",
       "   Recall_Student  Recall_Professional  Recall_Combined  ROC_AUC_Student  \\\n",
       "0        0.909262             0.744630         0.862789         0.816140   \n",
       "1        0.975907             0.887033         0.950820         0.755660   \n",
       "2        0.948373             0.792363         0.904334         0.729456   \n",
       "3        0.900188             0.738266         0.854480         0.806333   \n",
       "4        0.896746             0.652347         0.827757         0.835599   \n",
       "5        0.897997             0.661893         0.831350         0.835803   \n",
       "6        0.895181             0.622116         0.818100         0.840087   \n",
       "\n",
       "   ROC_AUC_Professional  ROC_AUC_Combined  \n",
       "0              0.853895          0.899963  \n",
       "1              0.899093          0.910628  \n",
       "2              0.861442          0.894672  \n",
       "3              0.847767          0.892611  \n",
       "4              0.819615          0.895810  \n",
       "5              0.824132          0.897332  \n",
       "6              0.805550          0.892489  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv('model_comparison_results.csv')\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
